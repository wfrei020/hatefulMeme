{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hateful_meme.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhWSrChmm3hFEKFEJhUPMN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfrei020/hatefulMeme/blob/master/hateful_meme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okT1TpGfDuVZ",
        "colab_type": "text"
      },
      "source": [
        "This notebook is the main training procedure for the hateful Meme classification, it will print out 2 models (BERT model and Image Model) plus a bert config. This is only for the training which i did in google colab.  The actual testing where i fuse the two results are in fuse_bert_resnet.py ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rXnfbq97vkd",
        "colab_type": "text"
      },
      "source": [
        "If i want to run this remeber to go through all the pre processing such as creating directories and downloading files etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jzbJiK7AsIf",
        "colab_type": "text"
      },
      "source": [
        "Download dataset , please email me directly to see if can get you an updated website to downoad the dataset if possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sOHy4YCx2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "bf6f49d2-c4d1-49b5-8d08-75c972a81b05"
      },
      "source": [
        "!wget -O Lnmwdnq3YcF7F3YsJncp.zip --no-check-certificate --no-proxy \"https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=Kku8%2BwVUQP%2BrKWVxFtoNpzi2dxU%3D&Expires=1596846254\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-06 23:43:20--  https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=Kku8%2BwVUQP%2BrKWVxFtoNpzi2dxU%3D&Expires=1596846254\n",
            "Resolving drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)... 52.218.237.27\n",
            "Connecting to drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)|52.218.237.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3599495834 (3.4G) [application/zip]\n",
            "Saving to: ‘Lnmwdnq3YcF7F3YsJncp.zip’\n",
            "\n",
            "Lnmwdnq3YcF7F3YsJnc 100%[===================>]   3.35G  37.3MB/s    in 59s     \n",
            "\n",
            "2020-08-06 23:44:20 (58.1 MB/s) - ‘Lnmwdnq3YcF7F3YsJncp.zip’ saved [3599495834/3599495834]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruqEKXrPBO18",
        "colab_type": "text"
      },
      "source": [
        "Unzip the deataset into directory data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xj5xwwbDWQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q -P KexZs4tn8hujn1nK Lnmwdnq3YcF7F3YsJncp.zip -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfyXmKSuBHTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "892ecc4b-0831-481f-8ca8-e8952554693f"
      },
      "source": [
        "ls -lrt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4439680\n",
            "-rw-r--r-- 1 root root 3599495834 Jun 13 00:36 Lnmwdnq3YcF7F3YsJncp.zip\n",
            "drwxr-xr-x 1 root root       4096 Jul 30 16:30 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root  464158224 Aug  6 15:45 hateful_meme.bin\n",
            "-rw-r--r-- 1 root root        313 Aug  6 15:45 hateful_meme_config_file.bin\n",
            "drwxr-xr-x 4 root root       4096 Aug  6 15:56 \u001b[01;34mdata\u001b[0m/\n",
            "-rw-r--r-- 1 root root  241270824 Aug  6 16:25 hateful_meme.model\n",
            "-rw-r--r-- 1 root root       1072 Aug  6 16:26 resnet152_config.py\n",
            "drwxr-xr-x 2 root root       4096 Aug  6 16:26 \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw-r--r-- 1 root root  241270824 Aug  6 17:57 hateful_img_meme.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlICl-WYBSwY",
        "colab_type": "text"
      },
      "source": [
        "The steps below are used for my image process generators..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtKqHlhRKeNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir hateful_nothateful_images\n",
        "!mkdir hateful_nothateful_images/training\n",
        "!mkdir hateful_nothateful_images/training/hateful\n",
        "!mkdir hateful_nothateful_images/training/not_hateful\n",
        "!mkdir hateful_nothateful_images/validation\n",
        "!mkdir hateful_nothateful_images/validation/hateful\n",
        "!mkdir hateful_nothateful_images/validation/not_hateful\n",
        "!mkdir hateful_nothateful_images/testing\n",
        "!mkdir hateful_nothateful_images/testing/hateful\n",
        "!mkdir hateful_nothateful_images/testing/not_hateful"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVX9Gg4yBdCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "5b83abc6-fc9b-43a7-c9bf-d80013af9c9c"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.33)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.33->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VXFHzJ_DpGO",
        "colab_type": "text"
      },
      "source": [
        "The main test training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33n82cEbB1Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "3093e1bc-1a3b-4c06-83e1-7007b07e295a"
      },
      "source": [
        "# verify GPU availability\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# matplotlib inline\n",
        "\n",
        "\n",
        "def checkGPU():\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name != '/device:GPU:0':\n",
        "        raise SystemError('GPU device not found')\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "def specifyDevice():\n",
        "  #  device = torch.device(\"cpu\")\n",
        "   # n_gpu = 0\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    torch.cuda.get_device_name(0)\n",
        "    return device, n_gpu\n",
        "\n",
        "\n",
        "def getTokenizer():\n",
        "    with open(\"data/data/train.jsonl\") as f:\n",
        "        lines = f.readlines()\n",
        "        labels = [json.loads(x)['label'] for x in lines]\n",
        "        ids = [json.loads(x)['id'] for x in lines]\n",
        "        bert_lines = [\"[CLS] \" + json.loads(x)['text'] + \" [SEP]\" for x in lines]\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in bert_lines]\n",
        "    print(\"Tokenize the first sentence:\")\n",
        "    print(tokenized_texts[0])\n",
        "\n",
        "    # Set the maximum sequence length.\n",
        "    MAX_LEN = 128\n",
        "    # Pad our input tokens\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    return attention_masks, input_ids, labels,ids, len(labels)\n",
        "\n",
        "\n",
        "def create_tensors_for_tuning():\n",
        "    attention_masks, input_ids, labels,ids, nb_labels = getTokenizer()\n",
        "    # Use train_test_split to split our data into train and validation sets for training\n",
        "    # don't need these since my data is alrady split\n",
        "    #train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "    #                                                                                    random_state=2018, test_size=0.1)\n",
        "    #train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "    #                                                       random_state=2018, test_size=0.1)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    attention_masks, input_ids, labels, ids, nb_labels = getTokenizer()\n",
        "    train_inputs = torch.tensor(input_ids)\n",
        "    train_labels = torch.tensor(labels)\n",
        "    train_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    # Select a batch size for training.\n",
        "    batch_size = 32\n",
        "\n",
        "    # Create an iterator of our data with torch DataLoader\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    model = BertFineTunning(train_data, train_sampler, train_dataloader, nb_labels)\n",
        "    output_model_file = \"./hateful_meme.bin\"\n",
        "    output_config_file = \"./hateful_meme_config_file.bin\"\n",
        "    output_vocab_file = \"./my_own_vocab_file.bin\"\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "    model_to_save.config.to_json_file(output_config_file)\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\n",
        "\n",
        "def BertFineTunning(train_data, train_sampler, train_dataloader, nb_labels):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    model.cuda()\n",
        "    # BERT fine-tuning parameters\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                         lr=2e-5,\n",
        "                         warmup=.1)\n",
        "\n",
        "    # Function to calculate the accuracy of our predictions vs labels\n",
        "    def flat_accuracy(preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    # Store our loss and accuracy for plotting\n",
        "    train_loss_set = []\n",
        "    # Number of training epochs\n",
        "    epochs = 6\n",
        "\n",
        "    # BERT training loop\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "        # TRAINING\n",
        "\n",
        "        # Set our model to training mode\n",
        "        model.train()\n",
        "        device, num_gpu = specifyDevice()\n",
        "        # Tracking variables\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        # Train the data for one epoch\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            # Clear out the gradients (by default they accumulate)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            train_loss_set.append(loss.item())\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            # Update tracking variables\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += b_input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "        print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
        "    return model\n",
        "\n",
        "\n",
        "create_tensors_for_tuning()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'its', 'their', 'character', 'not', 'their', 'color', 'that', 'matters', '[SEP]']\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'its', 'their', 'character', 'not', 'their', 'color', 'that', 'matters', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n",
            "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "Epoch:  17%|█▋        | 1/6 [06:52<34:21, 412.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.645890614031849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 2/6 [13:45<27:29, 412.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.5559583937091038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 3/6 [20:37<20:37, 412.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.488579330363668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 4/6 [27:27<13:43, 411.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.4156781526324444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  83%|████████▎ | 5/6 [34:17<06:51, 411.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.34747386017912313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 6/6 [41:08<00:00, 411.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.31116723941457003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4o9Yi4lL5TB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98f17c06-b7e5-4aa3-a508-a21a3786eb48"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                       Lnmwdnq3YcF7F3YsJncp.zip  \u001b[01;34msample_data\u001b[0m/\n",
            "\u001b[01;34mhateful_nothateful_images\u001b[0m/  resnet152_config.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY6AFNVneLpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "import resnet152_config as config\n",
        "#from imutils import paths\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "\n",
        "# grab the paths to all input images in the original input directory\n",
        "# and shuffle them\n",
        "count = 50\n",
        "\n",
        "with open(\"data/data/train.jsonl\") as f:\n",
        "    lines = f.readlines()\n",
        "    for x in lines:\n",
        "            if json.loads(x)['label']:\n",
        "                shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.TRAIN_PATH, \"hateful\"]))\n",
        "            else:\n",
        "                shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.TRAIN_PATH, \"not_hateful\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSWrZYn-hM4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "import resnet152_config as config\n",
        "#from imutils import paths\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "\n",
        "# grab the paths to all input images in the original input directory\n",
        "# and shuffle them\n",
        "count = 50\n",
        "\n",
        "with open(\"data/data/dev.jsonl\") as f:\n",
        "    lines = f.readlines()\n",
        "    for x in lines:\n",
        "            if json.loads(x)['label']:\n",
        "                shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.VAL_PATH, \"hateful\"]))\n",
        "            else:\n",
        "                shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.VAL_PATH, \"not_hateful\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q08fguRuiCQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##this wont work\n",
        "# # import the necessary packages\n",
        "# import resnet152_config as config\n",
        "# #from imutils import paths\n",
        "# import random\n",
        "# import shutil\n",
        "# import os\n",
        "# import json\n",
        "\n",
        "# # grab the paths to all input images in the original input directory\n",
        "# # and shuffle them\n",
        "# count = 50\n",
        "# #test.jsonl does not have labels LOL\n",
        "# with open(\"data/data/test.jsonl\") as f:\n",
        "#     lines = f.readlines()\n",
        "#     for x in lines:\n",
        "#             if json.loads(x)['label']:\n",
        "#                 shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.VAL_PATH, \"hateful\"]))\n",
        "#             else:\n",
        "#                 shutil.move(os.path.sep.join([\"data/data\", json.loads(x)['img']]), os.path.sep.join([config.VAL_PATH, \"not_hateful\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVLUEvXJDgRx",
        "colab_type": "text"
      },
      "source": [
        "The main image training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6E8s9spiU92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "1875ee01-3480-434b-d487-02dfe04af4d0"
      },
      "source": [
        "# based on https://www.pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "import resnet152_config as config\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.engine import data_adapter\n",
        "\n",
        "# let me load the data in a different manner\n",
        "# image_path = \"/Users/andrei/Ryerson/neuralNet_EE8204/codeTests/resnet/hateful_nothateful_images/validation/hateful/95640.png\"\n",
        "# image = load_img(image_path, target_size=(224, 224))\n",
        "# input_arr = img_to_array(image)\n",
        "# #input_arr = np.array([input_arr])\n",
        "# print(input_arr.shape)\n",
        "# exit()\n",
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "#                help=\"path to output loss/accuracy plot\")\n",
        "#args = vars(ap.parse_args())\n",
        "# determine the total number of image paths in training, validation,\n",
        "# and testing directories\n",
        "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
        "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
        "\n",
        "print(\"initializing the training data augmentation object\")\n",
        "# initialize the training training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")\n",
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "print(\"initializing the val data augmentation object\")\n",
        "valAug = ImageDataGenerator()\n",
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "trainAug.mean = mean\n",
        "valAug.mean = mean\n",
        "\n",
        "\n",
        "# initialize the training generator\n",
        "print(\"initializing the training generator\")\n",
        "trainGen = trainAug.flow_from_directory(\n",
        "    config.TRAIN_PATH,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=True,\n",
        "    batch_size=config.BS)\n",
        "# by the looks of it the train gnerator returns a dictinaryiterator [x,y] whre x is the batch and\n",
        "# y is the labels\n",
        "# print(len(trainGen.next()[1]))\n",
        "# initialize the validation generator\n",
        "print(\"initializing the val generator\")\n",
        "valGen = valAug.flow_from_directory(\n",
        "    config.VAL_PATH,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=False,\n",
        "    batch_size=config.BS)\n",
        "# initialize the testing generator\n",
        "print(\"initializing the testing generator\")\n",
        "testGen = valAug.flow_from_directory(\n",
        "    config.TEST_PATH,\n",
        "    class_mode=\"categorical\",\n",
        "    target_size=(224, 224),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=False,\n",
        "    batch_size=1)\n",
        "\n",
        "\n",
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = ResNet152(weights=\"imagenet\", include_top=False,\n",
        "                      input_tensor=Input(shape=(224, 224, 3)))\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "\n",
        "headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)  # this is my softmax layer for output\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "    # compile the model\n",
        "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit(\n",
        "    x=trainGen,\n",
        "    steps_per_epoch=totalTrain // config.BS,\n",
        "    validation_data=valGen,\n",
        "    validation_steps=totalVal // config.BS,\n",
        "    epochs=config.NUM_EPOCHS)\n",
        "\n",
        "# print(model.summary())\n",
        "#tf.keras.experimental.export_saved_model(model, config.MODEL_PATH)\n",
        "model.save(\"hateful_img_meme.model\", save_format=\"h5\")\n",
        "print(\"finished saving model\")\n",
        "exit()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initializing the training data augmentation object\n",
            "initializing the val data augmentation object\n",
            "initializing the training generator\n",
            "Found 8500 images belonging to 2 classes.\n",
            "initializing the val generator\n",
            "Found 500 images belonging to 2 classes.\n",
            "initializing the testing generator\n",
            "Found 0 images belonging to 2 classes.\n",
            "[INFO] preparing model...\n",
            "[INFO] training model...\n",
            "Epoch 1/20\n",
            "265/265 [==============================] - 214s 808ms/step - loss: 0.7873 - accuracy: 0.5882 - precision_2: 0.5882 - categorical_accuracy: 0.5882 - val_loss: 0.8144 - val_accuracy: 0.4708 - val_precision_2: 0.4708 - val_categorical_accuracy: 0.4708\n",
            "Epoch 2/20\n",
            "265/265 [==============================] - 213s 803ms/step - loss: 0.6523 - accuracy: 0.6321 - precision_2: 0.6321 - categorical_accuracy: 0.6321 - val_loss: 0.7895 - val_accuracy: 0.4854 - val_precision_2: 0.4854 - val_categorical_accuracy: 0.4854\n",
            "Epoch 3/20\n",
            "265/265 [==============================] - 214s 809ms/step - loss: 0.6438 - accuracy: 0.6422 - precision_2: 0.6422 - categorical_accuracy: 0.6422 - val_loss: 0.7809 - val_accuracy: 0.4750 - val_precision_2: 0.4750 - val_categorical_accuracy: 0.4750\n",
            "Epoch 4/20\n",
            "265/265 [==============================] - 215s 812ms/step - loss: 0.6318 - accuracy: 0.6493 - precision_2: 0.6493 - categorical_accuracy: 0.6493 - val_loss: 0.8689 - val_accuracy: 0.4812 - val_precision_2: 0.4812 - val_categorical_accuracy: 0.4812\n",
            "Epoch 5/20\n",
            "265/265 [==============================] - 216s 817ms/step - loss: 0.6261 - accuracy: 0.6497 - precision_2: 0.6497 - categorical_accuracy: 0.6497 - val_loss: 0.7872 - val_accuracy: 0.4833 - val_precision_2: 0.4833 - val_categorical_accuracy: 0.4833\n",
            "Epoch 6/20\n",
            "265/265 [==============================] - 218s 821ms/step - loss: 0.6216 - accuracy: 0.6572 - precision_2: 0.6572 - categorical_accuracy: 0.6572 - val_loss: 0.8495 - val_accuracy: 0.4875 - val_precision_2: 0.4875 - val_categorical_accuracy: 0.4875\n",
            "Epoch 7/20\n",
            "265/265 [==============================] - 219s 826ms/step - loss: 0.6151 - accuracy: 0.6653 - precision_2: 0.6653 - categorical_accuracy: 0.6653 - val_loss: 0.8000 - val_accuracy: 0.4792 - val_precision_2: 0.4792 - val_categorical_accuracy: 0.4792\n",
            "Epoch 8/20\n",
            "265/265 [==============================] - 219s 828ms/step - loss: 0.6088 - accuracy: 0.6636 - precision_2: 0.6636 - categorical_accuracy: 0.6636 - val_loss: 0.7893 - val_accuracy: 0.4833 - val_precision_2: 0.4833 - val_categorical_accuracy: 0.4833\n",
            "Epoch 9/20\n",
            "265/265 [==============================] - 220s 830ms/step - loss: 0.6029 - accuracy: 0.6752 - precision_2: 0.6752 - categorical_accuracy: 0.6752 - val_loss: 0.8059 - val_accuracy: 0.4875 - val_precision_2: 0.4875 - val_categorical_accuracy: 0.4875\n",
            "Epoch 10/20\n",
            "265/265 [==============================] - 220s 828ms/step - loss: 0.5982 - accuracy: 0.6778 - precision_2: 0.6778 - categorical_accuracy: 0.6778 - val_loss: 0.8110 - val_accuracy: 0.4875 - val_precision_2: 0.4875 - val_categorical_accuracy: 0.4875\n",
            "Epoch 11/20\n",
            "265/265 [==============================] - 221s 833ms/step - loss: 0.5935 - accuracy: 0.6850 - precision_2: 0.6850 - categorical_accuracy: 0.6850 - val_loss: 0.8316 - val_accuracy: 0.5000 - val_precision_2: 0.5000 - val_categorical_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "265/265 [==============================] - 217s 821ms/step - loss: 0.5915 - accuracy: 0.6808 - precision_2: 0.6808 - categorical_accuracy: 0.6808 - val_loss: 0.8078 - val_accuracy: 0.4979 - val_precision_2: 0.4979 - val_categorical_accuracy: 0.4979\n",
            "Epoch 13/20\n",
            "265/265 [==============================] - 218s 821ms/step - loss: 0.5852 - accuracy: 0.6873 - precision_2: 0.6873 - categorical_accuracy: 0.6873 - val_loss: 0.8304 - val_accuracy: 0.4875 - val_precision_2: 0.4875 - val_categorical_accuracy: 0.4875\n",
            "Epoch 14/20\n",
            "265/265 [==============================] - 218s 822ms/step - loss: 0.5849 - accuracy: 0.6872 - precision_2: 0.6872 - categorical_accuracy: 0.6872 - val_loss: 0.8504 - val_accuracy: 0.4833 - val_precision_2: 0.4833 - val_categorical_accuracy: 0.4833\n",
            "Epoch 15/20\n",
            "265/265 [==============================] - 218s 823ms/step - loss: 0.5780 - accuracy: 0.6913 - precision_2: 0.6913 - categorical_accuracy: 0.6913 - val_loss: 0.8212 - val_accuracy: 0.4792 - val_precision_2: 0.4792 - val_categorical_accuracy: 0.4792\n",
            "Epoch 16/20\n",
            "265/265 [==============================] - 218s 821ms/step - loss: 0.5707 - accuracy: 0.6956 - precision_2: 0.6956 - categorical_accuracy: 0.6956 - val_loss: 0.8567 - val_accuracy: 0.4938 - val_precision_2: 0.4938 - val_categorical_accuracy: 0.4938\n",
            "Epoch 17/20\n",
            "265/265 [==============================] - 217s 821ms/step - loss: 0.5671 - accuracy: 0.7021 - precision_2: 0.7021 - categorical_accuracy: 0.7021 - val_loss: 0.9127 - val_accuracy: 0.4917 - val_precision_2: 0.4917 - val_categorical_accuracy: 0.4917\n",
            "Epoch 18/20\n",
            "265/265 [==============================] - 218s 821ms/step - loss: 0.5645 - accuracy: 0.7084 - precision_2: 0.7084 - categorical_accuracy: 0.7084 - val_loss: 0.9512 - val_accuracy: 0.4875 - val_precision_2: 0.4875 - val_categorical_accuracy: 0.4875\n",
            "Epoch 19/20\n",
            "265/265 [==============================] - 219s 828ms/step - loss: 0.5556 - accuracy: 0.7081 - precision_2: 0.7081 - categorical_accuracy: 0.7081 - val_loss: 0.8646 - val_accuracy: 0.4938 - val_precision_2: 0.4938 - val_categorical_accuracy: 0.4938\n",
            "Epoch 20/20\n",
            "265/265 [==============================] - 221s 834ms/step - loss: 0.5538 - accuracy: 0.7091 - precision_2: 0.7091 - categorical_accuracy: 0.7091 - val_loss: 0.8717 - val_accuracy: 0.4938 - val_precision_2: 0.4938 - val_categorical_accuracy: 0.4938\n",
            "finished saving model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}